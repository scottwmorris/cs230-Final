{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = tf.constant(C, name=\"C\")\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(labels, depth=C, axis=0)\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "    sess.close()\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    parameters = {}\n",
    "    parameters['W1'] = tf.get_variable(\"W1\", [25,370], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    parameters['b1'] = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "    parameters['W2'] = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    parameters['b2'] = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "    parameters['W3'] = tf.get_variable(\"W3\", [1,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    parameters['b3'] = tf.get_variable(\"b3\", [1,1], initializer = tf.zeros_initializer())    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    Z1 = tf.add(tf.matmul(parameters['W1'], X), parameters['b1'])       # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                 # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(parameters['W2'], A1), parameters['b2'])  # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                 # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(parameters['W3'], A2), parameters['b3'])  # Z3 = np.dot(W3,Z2) + b3\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before reduce mean need to multiply labels by whatever weighting factor we want\n",
    "# same effect as having more 1 examples\n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_minibatches(X_train, Y_train, minibatch_size, num_minibatches, seed):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(X_train.shape[1])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    result = []\n",
    "    for idx in range(num_minibatches):\n",
    "        batch_X = np.zeros((X_train.shape[0], minibatch_size))\n",
    "        batch_Y = np.zeros((Y_train.shape[0], minibatch_size))\n",
    "        for batch_idx in range(minibatch_size):\n",
    "            shuffled_idx = indices[idx * minibatch_size + batch_idx]\n",
    "            batch_X[:, batch_idx] = X_train[:, shuffled_idx]\n",
    "            batch_Y[:, batch_idx] = Y_train[:, shuffled_idx]\n",
    "        result.append((batch_X, batch_Y))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_dev, Y_dev, learning_rate = 0.0001,\n",
    "          num_epochs = 10, minibatch_size = 32, print_cost = True):\n",
    "\n",
    "    X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "    m = X_train.shape[1]\n",
    "    seed = 12345\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            print \"At epoch\", epoch\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_minibatches(X_train, Y_train, minibatch_size, num_minibatches, seed)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "        parameters = sess.run(parameters)\n",
    "        \n",
    "        # Can change this to be AUC, precision, recall or whatever else we want\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_dev, Y: Y_dev}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n",
      "(60816, 370)\n",
      "(60816, 370)\n",
      "(60816, 1)\n",
      "(1, 60816)\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0\n",
      "At epoch 1\n",
      "At epoch 2\n",
      "At epoch 3\n",
      "At epoch 4\n",
      "At epoch 5\n",
      "At epoch 6\n",
      "At epoch 7\n",
      "At epoch 8\n",
      "At epoch 9\n",
      "('Train Accuracy:', 1.0)\n",
      "('Test Accuracy:', 1.0)\n",
      "{'W1': array([[-0.07948028,  0.09659575, -0.11876982, ...,  0.13776402,\n",
      "         0.01562993, -0.12454729],\n",
      "       [-0.01342665,  0.09074806, -0.01259688, ..., -0.07932034,\n",
      "        -0.09072158, -0.1069831 ],\n",
      "       [ 0.02849289,  0.03035812,  0.02753174, ..., -0.08598325,\n",
      "         0.02664629,  0.00406097],\n",
      "       ...,\n",
      "       [ 0.00675367,  0.11985517, -0.15145782, ..., -0.07126154,\n",
      "        -0.12832521, -0.0063318 ],\n",
      "       [ 0.06430473,  0.08779496, -0.18847886, ..., -0.02613265,\n",
      "         0.1119412 , -0.04295617],\n",
      "       [ 0.08628295,  0.02028473, -0.07185745, ...,  0.06814925,\n",
      "         0.04231611,  0.0009864 ]], dtype=float32), 'W3': array([[-0.33513495,  0.56492287, -0.60473925, -0.03221136,  0.4186695 ,\n",
      "        -0.6546142 , -0.5490231 , -0.07608661,  0.34030247,  0.315897  ,\n",
      "         0.36910373, -0.26492447]], dtype=float32), 'W2': array([[-2.13187560e-01,  3.25138360e-01, -3.62822235e-01,\n",
      "        -3.87053974e-02,  2.73761243e-01, -3.98682207e-01,\n",
      "        -3.26053768e-01, -3.19202505e-02,  2.11474001e-01,\n",
      "         1.69315577e-01,  2.14897364e-01, -1.45464435e-01,\n",
      "        -2.81621069e-01, -2.25219712e-01, -1.93468466e-01,\n",
      "        -3.56365810e-04, -8.02516937e-02, -1.95401132e-01,\n",
      "        -1.88087821e-01,  3.64447206e-01, -2.34178990e-01,\n",
      "         1.04978651e-01,  2.35086992e-01,  2.58869469e-01,\n",
      "        -4.24841613e-01],\n",
      "       [-6.18147738e-02, -9.88322645e-02, -5.91759980e-02,\n",
      "        -2.35263109e-01, -3.78387235e-02,  1.18376195e-01,\n",
      "         3.92685294e-01, -3.02811712e-01,  2.98809737e-01,\n",
      "         2.11757839e-01,  2.01361235e-02,  3.76648456e-01,\n",
      "         2.43709043e-01, -2.10010588e-01, -5.62376678e-02,\n",
      "        -2.22019047e-01, -1.21848077e-01,  2.98638374e-01,\n",
      "        -4.01220381e-01,  2.74632752e-01, -1.31208181e-01,\n",
      "        -2.69343346e-01,  3.10551114e-02,  2.03467354e-01,\n",
      "        -2.57206615e-02],\n",
      "       [ 1.79822132e-01,  1.52348116e-01, -3.00896853e-01,\n",
      "         1.76244169e-01, -2.72706151e-01, -3.15819085e-01,\n",
      "         3.48951340e-01, -8.55613947e-02, -2.45612428e-01,\n",
      "        -4.00712729e-01, -3.31155330e-01, -5.05566411e-02,\n",
      "        -3.57175656e-02,  3.68870437e-01, -6.58533797e-02,\n",
      "        -1.33728683e-01,  5.18991500e-02,  1.22262063e-02,\n",
      "        -3.66142124e-01,  1.20067019e-02, -3.54602516e-01,\n",
      "        -2.83142656e-01,  3.32992643e-01,  1.73289463e-01,\n",
      "         3.27537447e-01],\n",
      "       [ 1.14459328e-01,  1.91219062e-01,  1.80346563e-01,\n",
      "         1.80161059e-01,  6.20687902e-02, -5.82601391e-02,\n",
      "         2.57636547e-01, -2.96092987e-01,  3.08627216e-03,\n",
      "         2.58570537e-02, -5.07381409e-02,  3.01357239e-01,\n",
      "        -3.09446275e-01, -1.38763443e-01, -2.02630222e-01,\n",
      "        -3.46785069e-01,  1.95437163e-01,  2.77961135e-01,\n",
      "         1.59718618e-02,  2.17747495e-01,  3.53299350e-01,\n",
      "        -2.36236736e-01,  2.23887116e-01, -4.16959196e-01,\n",
      "         2.88644791e-01],\n",
      "       [ 2.83898205e-01, -3.81868154e-01, -3.59369040e-01,\n",
      "         2.03499168e-01,  3.08743268e-01,  1.56388760e-01,\n",
      "         2.13609606e-01, -1.86453894e-01, -2.03086689e-01,\n",
      "        -2.80551255e-01, -3.26321304e-01,  9.28009748e-02,\n",
      "         2.69816160e-01, -1.97371781e-01, -7.93625265e-02,\n",
      "         2.90739059e-01, -1.17674284e-01,  1.94572017e-01,\n",
      "         1.78793162e-01,  2.11028680e-01,  3.46670151e-01,\n",
      "        -2.09452182e-01,  1.39530405e-01, -2.44592756e-01,\n",
      "         3.36140513e-01],\n",
      "       [ 8.65947306e-02, -1.08375639e-01, -3.56539607e-01,\n",
      "         4.09321934e-01, -1.48750722e-01, -3.12540412e-01,\n",
      "        -3.34444821e-01,  1.39760852e-01, -4.45747338e-02,\n",
      "         3.78744841e-01,  2.21111011e-02,  2.34291509e-01,\n",
      "         1.83945283e-01,  9.30012539e-02,  2.29900658e-01,\n",
      "         4.18443263e-01, -1.72870979e-01, -8.78614187e-02,\n",
      "         3.04071546e-01, -2.90449500e-01,  1.29517257e-01,\n",
      "         1.56808496e-01,  2.92780697e-01,  3.43766123e-01,\n",
      "        -4.81521618e-03],\n",
      "       [ 3.19779962e-01, -3.95744771e-01, -3.93114239e-01,\n",
      "         5.40335998e-02, -3.38541895e-01, -2.09964607e-02,\n",
      "        -5.13029844e-02,  3.04821312e-01,  3.15148294e-01,\n",
      "         3.07615399e-01, -3.09232354e-01,  1.32008949e-02,\n",
      "        -2.80232251e-01,  3.26654553e-01, -6.98252618e-02,\n",
      "         2.03523878e-02, -5.96800596e-02,  3.05256784e-01,\n",
      "        -9.63254273e-02,  3.69321951e-03, -2.94211894e-01,\n",
      "         1.20420858e-01, -1.23161867e-01,  1.89556330e-01,\n",
      "         1.39482524e-02],\n",
      "       [-3.57300699e-01,  2.46099159e-01,  1.88654825e-01,\n",
      "        -3.56960386e-01, -5.84119149e-02,  2.71279991e-01,\n",
      "        -3.35794449e-01,  3.73860806e-01,  1.33390445e-02,\n",
      "         3.78272057e-01, -5.23701720e-02,  1.04359731e-01,\n",
      "         1.66349009e-01, -3.47287953e-01, -3.20012644e-02,\n",
      "         1.74896926e-01, -8.65767971e-02,  8.90409499e-02,\n",
      "         4.01133686e-01,  4.38310504e-01,  1.73332825e-01,\n",
      "        -2.65497178e-01,  3.57674770e-02,  1.97346851e-01,\n",
      "        -1.47441924e-01],\n",
      "       [-5.27048297e-02, -1.35725871e-01,  4.23165113e-02,\n",
      "         6.68173134e-02, -1.20393947e-01, -3.45512807e-01,\n",
      "         1.04108520e-01, -8.22585300e-02,  2.63274252e-01,\n",
      "         2.98819710e-02,  2.49381557e-01, -7.68970326e-02,\n",
      "         2.07068712e-01, -1.76532730e-01, -1.32078141e-01,\n",
      "        -1.63262069e-01,  1.01978131e-01,  9.60926786e-02,\n",
      "         3.80349845e-01, -2.97636420e-01, -3.42946410e-01,\n",
      "        -2.93377131e-01,  3.17460783e-02, -3.07865590e-01,\n",
      "         2.87679642e-01],\n",
      "       [-2.27393582e-01, -4.16279048e-01,  1.20551020e-01,\n",
      "        -1.12628654e-01, -9.26897004e-02,  1.51054516e-01,\n",
      "         2.16317058e-01,  3.89772147e-01,  2.42050793e-02,\n",
      "         2.84081042e-01,  3.17496508e-01, -1.56197235e-01,\n",
      "         2.92414159e-01, -1.38616517e-01,  2.55796071e-02,\n",
      "        -3.72632816e-02, -8.77564251e-02, -3.85031402e-01,\n",
      "        -1.41158268e-01,  4.68015634e-02, -2.55299974e-02,\n",
      "         1.20791994e-01,  3.06795329e-01,  1.48496792e-01,\n",
      "        -2.29428306e-01],\n",
      "       [-2.71190912e-01, -2.10404605e-01,  2.65622348e-01,\n",
      "         3.30325246e-01, -6.47139037e-03, -3.62242311e-01,\n",
      "        -1.78386699e-02,  1.12735905e-01,  3.60138446e-01,\n",
      "         3.48824829e-01, -2.22047240e-01, -3.11970413e-01,\n",
      "         3.44417870e-01, -3.23606253e-01, -2.91470647e-01,\n",
      "         1.20635342e-03,  2.04121005e-02,  3.51818562e-01,\n",
      "        -3.35532159e-01, -4.02348727e-01, -9.27489530e-03,\n",
      "        -4.00817811e-01, -3.68113637e-01, -5.42955473e-03,\n",
      "         1.90605506e-01],\n",
      "       [-1.11197300e-01,  3.55025887e-01,  3.88388425e-01,\n",
      "         2.99392253e-01,  1.37228996e-01,  1.56281725e-01,\n",
      "         2.25363597e-01, -8.10948238e-02, -3.04274499e-01,\n",
      "         1.95806906e-01,  1.44239798e-01, -8.77898484e-02,\n",
      "        -3.57648611e-01,  2.99499452e-01,  1.19116560e-01,\n",
      "        -3.39284360e-01,  9.44363922e-02, -1.99088439e-01,\n",
      "         2.65515834e-01, -3.32591206e-01, -1.65724307e-01,\n",
      "        -2.08280727e-01,  3.85039449e-01,  3.41249466e-01,\n",
      "         3.74973655e-01]], dtype=float32), 'b2': array([[-0.05531964],\n",
      "       [-0.07822409],\n",
      "       [-0.01183711],\n",
      "       [-0.34959805],\n",
      "       [-0.02383161],\n",
      "       [ 0.01017743],\n",
      "       [-0.01046694],\n",
      "       [ 0.08109375],\n",
      "       [-0.04352134],\n",
      "       [-0.06632115],\n",
      "       [-0.07717283],\n",
      "       [ 0.00847702]], dtype=float32), 'b3': array([[0.30312407]], dtype=float32), 'b1': array([[-0.00694582],\n",
      "       [ 0.03428361],\n",
      "       [ 0.03676943],\n",
      "       [-0.00211667],\n",
      "       [-0.01232079],\n",
      "       [-0.05042019],\n",
      "       [-0.03687969],\n",
      "       [ 0.01944981],\n",
      "       [-0.04860493],\n",
      "       [ 0.00745791],\n",
      "       [ 0.06489342],\n",
      "       [-0.01307683],\n",
      "       [-0.06806973],\n",
      "       [-0.00597805],\n",
      "       [ 0.05969243],\n",
      "       [ 0.09341528],\n",
      "       [-0.00378417],\n",
      "       [-0.03173788],\n",
      "       [-0.0158457 ],\n",
      "       [ 0.02079648],\n",
      "       [-0.03615301],\n",
      "       [ 0.04230446],\n",
      "       [ 0.02223293],\n",
      "       [ 0.0365004 ],\n",
      "       [-0.08245522]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# Load Training Data\n",
    "training_data = np.load(\"Train_Set.npy\")\n",
    "dev_data = np.load(\"Dev_Set.npy\")\n",
    "\n",
    "# Separate data into features and labels\n",
    "num_feats = training_data.shape[1]\n",
    "training_feats = training_data[:, 0:num_feats-1]\n",
    "training_labels = training_data[:, num_feats-1]\n",
    "training_labels = np.reshape(training_labels, (training_labels.shape[0],1))\n",
    "dev_feats = dev_data[:, 0:num_feats-1]\n",
    "dev_labels = dev_data[:, num_feats-1]\n",
    "dev_labels = np.reshape(dev_labels, (dev_labels.shape[0],1))\n",
    "\n",
    "print (num_feats)\n",
    "print (training_feats.shape)\n",
    "\n",
    "X_train = np.transpose(training_feats)\n",
    "X_dev = np.transpose(dev_feats)\n",
    "\n",
    "Y_train = np.transpose(training_labels)\n",
    "Y_dev = np.transpose(dev_labels)\n",
    "\n",
    "print (training_feats.shape)\n",
    "print (training_labels.shape)\n",
    "print (Y_train.shape)\n",
    "print (Y_train)\n",
    "\n",
    "# Basically all we need to do is format in the following:\n",
    "\n",
    "# number of training examples = 1080\n",
    "# number of test examples = 120\n",
    "# X_train shape: (num_features, num_examples)\n",
    "# Y_train shape: (2 (one hot vectors), num_examples)\n",
    "# X_test shape: (12288, 120)\n",
    "# Y_test shape: (2 (one hot vectors, num_dev_examples)\n",
    "\n",
    "# and then feed into \n",
    "parameters = model(X_train, Y_train, X_dev, Y_dev)\n",
    "print parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
