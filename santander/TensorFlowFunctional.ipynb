{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = tf.constant(C, name=\"C\")\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(labels, depth=C, axis=0)\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "    sess.close()\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    parameters = {}\n",
    "    parameters['W1'] = tf.get_variable(\"W1\", [25,370], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    parameters['b1'] = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "    parameters['W2'] = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    parameters['b2'] = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "    parameters['W3'] = tf.get_variable(\"W3\", [2,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    parameters['b3'] = tf.get_variable(\"b3\", [2,1], initializer = tf.zeros_initializer())    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    Z1 = tf.add(tf.matmul(parameters['W1'], X), parameters['b1'])       # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                 # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(parameters['W2'], A1), parameters['b2'])  # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                 # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(parameters['W3'], A2), parameters['b3'])  # Z3 = np.dot(W3,Z2) + b3\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_minibatches(X_train, Y_train, minibatch_size, num_minibatches, seed):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(X_train.shape[1])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    result = []\n",
    "    for idx in range(num_minibatches):\n",
    "        batch_X = np.zeros((X_train.shape[0], minibatch_size))\n",
    "        batch_Y = np.zeros((Y_train.shape[0], minibatch_size))\n",
    "        for batch_idx in range(minibatch_size):\n",
    "            shuffled_idx = indices[idx * minibatch_size + batch_idx]\n",
    "            batch_X[:, batch_idx] = X_train[:, shuffled_idx]\n",
    "            batch_Y[:, batch_idx] = Y_train[:, shuffled_idx]\n",
    "        result.append((batch_X, batch_Y))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 10, minibatch_size = 32, print_cost = True):\n",
    "\n",
    "    X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "    m = X_train.shape[1]\n",
    "    seed = 12345\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            print \"At epoch\", epoch\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_minibatches(X_train, Y_train, minibatch_size, num_minibatches, seed)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "        parameters = sess.run(parameters)\n",
    "        \n",
    "        # Can change this to be AUC, precision, recall or whatever else we want\n",
    "        #correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "        #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        #print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        #print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n",
      "(60816, 370)\n",
      "(60816, 370)\n",
      "(60816,)\n",
      "(2, 60816)\n",
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0\n",
      "At epoch 1\n",
      "At epoch 2\n",
      "At epoch 3\n",
      "At epoch 4\n",
      "At epoch 5\n",
      "At epoch 6\n",
      "At epoch 7\n",
      "At epoch 8\n",
      "At epoch 9\n",
      "{'W1': array([[-0.0616337 ,  0.11629868, -0.10080542, ...,  0.12544283,\n",
      "         0.00368036, -0.10253091],\n",
      "       [-0.02927052,  0.01062265, -0.10355762, ..., -0.09029795,\n",
      "        -0.1016992 , -0.08060744],\n",
      "       [ 0.00548588, -0.00306089,  0.12875566, ..., -0.06941327,\n",
      "         0.04197906, -0.01015502],\n",
      "       ...,\n",
      "       [ 0.02731596,  0.04076566, -0.03250039, ..., -0.06611993,\n",
      "        -0.12192569,  0.00893123],\n",
      "       [ 0.09449975,  0.00581689, -0.01396315, ..., -0.0535288 ,\n",
      "         0.08413698, -0.0055581 ],\n",
      "       [ 0.0745432 ,  0.06603334,  0.003219  , ...,  0.06401525,\n",
      "         0.04215014,  0.04263157]], dtype=float32), 'W3': array([[-0.29584777,  0.5528494 , -0.5863899 ,  0.0065484 ,  0.45439097,\n",
      "        -0.57649815, -0.5194648 , -0.02165317,  0.35297337,  0.34297332,\n",
      "         0.3802049 , -0.22680199],\n",
      "       [-0.5016487 , -0.3915937 , -0.25301164,  0.04044494, -0.079284  ,\n",
      "        -0.36664888, -0.28395644,  0.58394045, -0.39518827,  0.24420139,\n",
      "         0.39796808,  0.4154275 ]], dtype=float32), 'W2': array([[-0.18486504,  0.3744854 , -0.33524692,  0.00570755,  0.25487787,\n",
      "        -0.5130322 , -0.3652797 , -0.01369049,  0.22971778,  0.22745201,\n",
      "         0.22535953, -0.19119366, -0.24438925, -0.20711496, -0.10358905,\n",
      "         0.06246433, -0.00670086, -0.17791332, -0.1461733 ,  0.41176066,\n",
      "        -0.20770699,  0.19977875,  0.30157647,  0.3037762 , -0.38008595],\n",
      "       [-0.06139282, -0.0834605 , -0.03893038, -0.2319163 , -0.02208521,\n",
      "         0.13587072,  0.3929447 , -0.28607577,  0.30620396,  0.23307793,\n",
      "         0.02750508,  0.3873529 ,  0.25429407, -0.19934969, -0.01377747,\n",
      "        -0.22922696, -0.08697218,  0.30584922, -0.40267488,  0.31738824,\n",
      "        -0.08317151, -0.25420243,  0.04965034,  0.20983759, -0.02008109],\n",
      "       [ 0.16215706,  0.15862344, -0.31307632,  0.18252452, -0.259168  ,\n",
      "        -0.3385505 ,  0.3555133 , -0.10235833, -0.24270396, -0.39468837,\n",
      "        -0.35193223, -0.06471018, -0.03765607,  0.37370634, -0.06219273,\n",
      "        -0.09471117,  0.04525948, -0.01128048, -0.33895442,  0.03213929,\n",
      "        -0.37225354, -0.30357954,  0.3383778 ,  0.1802777 ,  0.29790628],\n",
      "       [ 0.10762782,  0.15478185,  0.14795604,  0.14933093,  0.04212829,\n",
      "        -0.09768592,  0.23959206, -0.31373337, -0.00931049,  0.01714598,\n",
      "        -0.05956651,  0.27253082, -0.34510642, -0.14852978, -0.24713476,\n",
      "        -0.37058753,  0.15635405,  0.23764089, -0.01501816,  0.15690167,\n",
      "         0.2879446 , -0.251599  ,  0.20755652, -0.42308316,  0.27894565],\n",
      "       [ 0.30335975, -0.36429057, -0.32917264,  0.23361316,  0.3291794 ,\n",
      "         0.18400672,  0.22817127, -0.15904883, -0.20246503, -0.29470134,\n",
      "        -0.32015502,  0.11528844,  0.29614216, -0.2034191 , -0.03219992,\n",
      "         0.33105078, -0.09652934,  0.2485852 ,  0.20868196,  0.24059907,\n",
      "         0.37778836, -0.18304686,  0.14609782, -0.24707659,  0.34348327],\n",
      "       [ 0.07011483, -0.16131426, -0.37588957,  0.3793398 , -0.16929899,\n",
      "        -0.33213836, -0.36198106,  0.11269495, -0.04630114,  0.3704329 ,\n",
      "         0.02169046,  0.21033253,  0.14032204,  0.07187476,  0.16966997,\n",
      "         0.37875235, -0.1963536 , -0.13702688,  0.28209552, -0.3498987 ,\n",
      "         0.07385187,  0.1329342 ,  0.26836267,  0.32260567, -0.01225403],\n",
      "       [ 0.31236854, -0.4115265 , -0.3967995 ,  0.05917569, -0.35528657,\n",
      "        -0.02154043, -0.08904468,  0.28277704,  0.32173836,  0.2861716 ,\n",
      "        -0.35655916,  0.00988983, -0.29172578,  0.33604312, -0.0575185 ,\n",
      "        -0.01027222, -0.03019498,  0.28988528, -0.10847306,  0.10556268,\n",
      "        -0.32751748,  0.11045181, -0.13753302,  0.18702745,  0.01224372],\n",
      "       [-0.37381664,  0.16513532,  0.15702054, -0.3927481 , -0.07975902,\n",
      "         0.237162  , -0.3939664 ,  0.34586686, -0.00472981,  0.36990696,\n",
      "        -0.05247652,  0.07787082,  0.11545457, -0.37145352, -0.09524764,\n",
      "         0.13432632, -0.11846594,  0.04063012,  0.37118107,  0.380085  ,\n",
      "         0.10932329, -0.29354474, -0.00537587,  0.20011882, -0.15558523],\n",
      "       [-0.06097218, -0.12112767,  0.06931233,  0.08937071, -0.11314763,\n",
      "        -0.32903448,  0.11263942, -0.06416117,  0.28098714,  0.01580312,\n",
      "         0.2579796 , -0.0566728 ,  0.22875158, -0.19105631, -0.14996336,\n",
      "        -0.1448163 ,  0.12801273,  0.13099767,  0.404162  , -0.26995406,\n",
      "        -0.33603492, -0.27403986,  0.03782484, -0.317673  ,  0.26981997],\n",
      "       [-0.22184852, -0.3802417 ,  0.14525305, -0.09030288, -0.08717994,\n",
      "         0.16229692,  0.2371656 ,  0.41863498,  0.03218222,  0.29031116,\n",
      "         0.31805277, -0.1403864 ,  0.31977928, -0.1312738 ,  0.11144434,\n",
      "        -0.00409612, -0.0601218 , -0.3467065 , -0.1226086 ,  0.09581872,\n",
      "         0.00977857,  0.14756863,  0.34102932,  0.16320236, -0.25107238],\n",
      "       [-0.28495166, -0.2016868 ,  0.2585142 ,  0.3366543 , -0.0118776 ,\n",
      "        -0.35346973, -0.0051497 ,  0.11242988,  0.36996433,  0.32452956,\n",
      "        -0.20762008, -0.297496  ,  0.3543241 , -0.33004504, -0.32807827,\n",
      "         0.00216266,  0.02530203,  0.36203066, -0.33561307, -0.37226495,\n",
      "        -0.02752533, -0.4114511 , -0.39605597, -0.02292914,  0.16819872],\n",
      "       [-0.1156042 ,  0.32005447,  0.35761705,  0.26995704,  0.11916512,\n",
      "         0.12280951,  0.21087164, -0.09919771, -0.31142652,  0.18724768,\n",
      "         0.14555569, -0.11314429, -0.3910246 ,  0.2898084 ,  0.0762192 ,\n",
      "        -0.3631954 ,  0.05927323, -0.237769  ,  0.23668034, -0.38918573,\n",
      "        -0.22890665, -0.22247441,  0.3681684 ,  0.33277747,  0.36762252]],\n",
      "      dtype=float32), 'b2': array([[-0.0049671 ],\n",
      "       [ 0.02641919],\n",
      "       [-0.03282743],\n",
      "       [-0.00595153],\n",
      "       [ 0.03590089],\n",
      "       [-0.02417015],\n",
      "       [ 0.01255513],\n",
      "       [-0.02258973],\n",
      "       [ 0.01716839],\n",
      "       [-0.01367852],\n",
      "       [-0.0209353 ],\n",
      "       [-0.03240902]], dtype=float32), 'b3': array([[ 0.01965173],\n",
      "       [-0.01965173]], dtype=float32), 'b1': array([[ 0.0127722 ],\n",
      "       [-0.03468029],\n",
      "       [-0.02188074],\n",
      "       [-0.01908915],\n",
      "       [ 0.02747566],\n",
      "       [ 0.00393641],\n",
      "       [-0.02125035],\n",
      "       [-0.03930176],\n",
      "       [ 0.03194296],\n",
      "       [-0.02753804],\n",
      "       [-0.0140106 ],\n",
      "       [ 0.02231652],\n",
      "       [-0.00587842],\n",
      "       [-0.01404325],\n",
      "       [-0.00030386],\n",
      "       [ 0.03277395],\n",
      "       [-0.05229457],\n",
      "       [ 0.01793505],\n",
      "       [-0.01429857],\n",
      "       [ 0.04044011],\n",
      "       [ 0.03865107],\n",
      "       [ 0.00173826],\n",
      "       [-0.01762781],\n",
      "       [-0.02888958],\n",
      "       [-0.01204066]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# Load Training Data\n",
    "training_data = np.load(\"Train_Set.npy\")\n",
    "dev_data = np.load(\"Dev_Set.npy\")\n",
    "\n",
    "# Separate data into features and labels\n",
    "num_feats = training_data.shape[1]\n",
    "training_feats = training_data[:, 0:num_feats-1]\n",
    "training_labels = training_data[:, num_feats-1]\n",
    "dev_feats = dev_data[:, 0:num_feats-1]\n",
    "dev_labels = dev_data[:, num_feats-1]\n",
    "\n",
    "print (num_feats)\n",
    "print (training_feats.shape)\n",
    "\n",
    "# ready to go but check shapes\n",
    "X_train = np.transpose(training_feats)\n",
    "X_dev = np.transpose(dev_feats)\n",
    "\n",
    "# still needs work, trying to make one hot matrix for train and dev labels\n",
    "Y_train = one_hot_matrix(training_labels, 2)\n",
    "Y_dev = one_hot_matrix(dev_labels, 2)\n",
    "\n",
    "print (training_feats.shape)\n",
    "print (training_labels.shape)\n",
    "print (Y_train.shape)\n",
    "print (Y_train)\n",
    "\n",
    "# Basically all we need to do is format in the following:\n",
    "\n",
    "# number of training examples = 1080\n",
    "# number of test examples = 120\n",
    "# X_train shape: (num_features, num_examples)\n",
    "# Y_train shape: (2 (one hot vectors), num_examples)\n",
    "# X_test shape: (12288, 120)\n",
    "# Y_test shape: (2 (one hot vectors, num_dev_examples)\n",
    "\n",
    "# and then feed into \n",
    "parameters = model(X_train, Y_train, X_dev, Y_dev)\n",
    "print parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
